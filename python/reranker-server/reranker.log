2025-05-10 17:45:35,679 - arxiv_reranker - INFO - Loading cross-encoder model: cross-encoder/ms-marco-MiniLM-L-6-v2
2025-05-10 17:45:35,681 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-10 17:45:35,802 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-05-10 17:45:35,846 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 200 0
2025-05-10 17:45:35,971 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2025-05-10 17:45:36,020 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2025-05-10 17:45:36,048 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cpu
2025-05-10 17:45:36,048 - arxiv_reranker - INFO - Model loaded successfully
2025-05-10 17:45:36,056 - arxiv_reranker - INFO - Starting ArXiv Paper Reranker API...
2025-05-10 17:45:36,056 - arxiv_reranker - INFO - Loading cross-encoder model: cross-encoder/ms-marco-MiniLM-L-6-v2
2025-05-10 17:45:36,056 - arxiv_reranker - INFO - Visit http://localhost:8000/docs for API documentation
2025-05-10 17:45:36,094 - arxiv_reranker - INFO - Model loaded successfully. Test prediction: -2.6098
2025-05-10 17:45:52,742 - arxiv_reranker - INFO - Loading cross-encoder model: cross-encoder/ms-marco-MiniLM-L-6-v2
2025-05-10 17:45:52,745 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-10 17:45:52,819 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-05-10 17:45:52,877 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 200 0
2025-05-10 17:45:52,966 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2025-05-10 17:45:53,014 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2025-05-10 17:45:53,032 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cpu
2025-05-10 17:45:53,032 - arxiv_reranker - INFO - Model loaded successfully
2025-05-10 17:45:53,042 - arxiv_reranker - INFO - Starting ArXiv Paper Reranker API...
2025-05-10 17:45:53,042 - arxiv_reranker - INFO - Loading cross-encoder model: cross-encoder/ms-marco-MiniLM-L-6-v2
2025-05-10 17:45:53,042 - arxiv_reranker - INFO - Visit http://localhost:8000/docs for API documentation
2025-05-10 17:45:53,053 - arxiv_reranker - INFO - Model loaded successfully. Test prediction: -2.6098
2025-05-10 18:06:53,769 - arxiv_reranker - DEBUG - Returning models information with current model: cross-encoder/ms-marco-MiniLM-L-6-v2
2025-05-10 18:07:17,688 - arxiv_reranker - INFO - Loading cross-encoder model: cross-encoder/ms-marco-MiniLM-L-6-v2
2025-05-10 18:07:17,689 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-10 18:07:17,880 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-05-10 18:07:17,929 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 200 0
2025-05-10 18:07:18,013 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2025-05-10 18:07:18,058 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2025-05-10 18:07:18,075 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cpu
2025-05-10 18:07:18,075 - arxiv_reranker - INFO - Model loaded successfully
2025-05-10 18:07:18,084 - arxiv_reranker - INFO - Starting ArXiv Paper Reranker API...
2025-05-10 18:07:18,084 - arxiv_reranker - INFO - Loading cross-encoder model: cross-encoder/ms-marco-MiniLM-L-6-v2
2025-05-10 18:07:18,084 - arxiv_reranker - INFO - Visit http://localhost:8000/docs for API documentation
2025-05-10 18:07:18,095 - arxiv_reranker - INFO - Model loaded successfully. Test prediction: -2.6098
2025-05-10 18:07:44,643 - arxiv_reranker - INFO - Loading cross-encoder model: cross-encoder/ms-marco-MiniLM-L-6-v2
2025-05-10 18:07:44,644 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-10 18:07:45,118 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-05-10 18:07:45,159 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 200 0
2025-05-10 18:07:45,241 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2025-05-10 18:09:20,419 - arxiv_reranker - INFO - Starting ArXiv Paper Reranker API using logging level DEBUG and log file reranker.log
2025-05-10 18:09:20,420 - arxiv_reranker - INFO - Loading cross-encoder model: cross-encoder/ms-marco-MiniLM-L-6-v2
2025-05-10 18:09:20,426 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-10 18:09:20,506 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-05-10 18:09:20,549 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 200 0
2025-05-10 18:09:20,702 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2025-05-10 18:09:20,753 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2025-05-10 18:09:20,773 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cpu
2025-05-10 18:09:20,773 - arxiv_reranker - INFO - Model loaded successfully
2025-05-10 18:09:20,798 - arxiv_reranker - INFO - Starting ArXiv Paper Reranker API...
2025-05-10 18:09:20,798 - arxiv_reranker - INFO - Loading cross-encoder model: cross-encoder/ms-marco-MiniLM-L-6-v2
2025-05-10 18:09:20,798 - arxiv_reranker - INFO - Visit http://localhost:8000/docs for API documentation
2025-05-10 18:09:20,818 - arxiv_reranker - INFO - Model loaded successfully. Test prediction: -2.6098
2025-05-10 18:09:34,803 - arxiv_reranker - DEBUG - Returning models information with current model: cross-encoder/ms-marco-MiniLM-L-6-v2
2025-05-10 18:18:51,984 - arxiv_reranker - INFO - Starting ArXiv Paper Reranker API using logging level DEBUG and log file reranker.log
2025-05-10 18:18:51,984 - arxiv_reranker - INFO - Loading cross-encoder model: cross-encoder/ms-marco-MiniLM-L-6-v2
2025-05-10 18:18:51,986 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-10 18:18:52,081 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-05-10 18:18:52,162 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 200 0
2025-05-10 18:18:52,249 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2025-05-10 18:18:52,292 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2025-05-10 18:18:52,308 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cpu
2025-05-10 18:18:52,309 - arxiv_reranker - INFO - Model loaded successfully
2025-05-10 18:18:52,317 - arxiv_reranker - INFO - Starting ArXiv Paper Reranker API...
2025-05-10 18:18:52,317 - arxiv_reranker - INFO - Loading cross-encoder model: cross-encoder/ms-marco-MiniLM-L-6-v2
2025-05-10 18:18:52,317 - arxiv_reranker - INFO - Visit http://localhost:8000/docs for API documentation
2025-05-10 18:18:52,328 - arxiv_reranker - INFO - Model loaded successfully. Test prediction: -2.6098
2025-05-10 18:19:17,785 - arxiv_reranker - INFO - Starting ArXiv Paper Reranker API using logging level DEBUG and log file reranker.log
2025-05-10 18:19:17,786 - arxiv_reranker - INFO - Loading cross-encoder model: cross-encoder/ms-marco-MiniLM-L-6-v2
2025-05-10 18:19:17,787 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-10 18:19:17,984 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-05-10 18:19:18,028 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 200 0
2025-05-10 18:19:18,189 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2025-05-10 18:19:18,231 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2025-05-10 18:19:18,251 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cpu
2025-05-10 18:19:18,252 - arxiv_reranker - INFO - Model loaded successfully
2025-05-10 18:19:18,261 - arxiv_reranker - INFO - Starting ArXiv Paper Reranker API...
2025-05-10 18:19:18,261 - arxiv_reranker - INFO - Loading cross-encoder model: cross-encoder/ms-marco-MiniLM-L-6-v2
2025-05-10 18:19:18,261 - arxiv_reranker - INFO - Visit http://localhost:8000/docs for API documentation
2025-05-10 18:19:18,272 - arxiv_reranker - INFO - Model loaded successfully. Test prediction: -2.6098
2025-05-10 18:19:26,291 - arxiv_reranker - DEBUG - Returning models information with current model: cross-encoder/ms-marco-MiniLM-L-6-v2
2025-05-10 18:19:34,909 - arxiv_reranker - DEBUG - Request body: {"query":"cross encoders for neural retrieval","results":[{"Title":"Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models","Authors":["Oliver Savolainen","Dur e Najaf Amjad","Roxana Petcu"],"Abstract":"This reproducibility study analyzes and extends the paper 'Interpreting Content-Based Neural Retrieval Models: What do they learn and when do they fail?' (Faggioli et al., 2022). We focus on investigating how neural retrieval models perform across different languages and document lengths. Our findings confirm the original paper's conclusions that neural models prioritize term frequency and document length features, but we extend this understanding to show how these models struggle with very long documents and exhibit biases in multilingual settings. This work contributes to improving information retrieval systems' performance across diverse content scenarios.","Published":"2025-05-04T15:30:45Z","PDFURL":"http://arxiv.org/pdf/2505.02154v1","SourceURL":"http://arxiv.org/abs/2505.02154v1","Metadata":{"primary_category":"cs.IR"}},{"Title":"Reranking for Neural Information Retrieval: A Comprehensive Survey of Approaches and Applications","Authors":["Emma Chen","Javier Rodriguez","Sarah Johnson"],"Abstract":"Reranking is a crucial step in modern information retrieval systems, significantly improving the quality of search results by refining an initial candidate set. This survey provides a systematic overview of neural reranking approaches, from early neural network models to recent pretrained language model-based techniques. We categorize reranking approaches based on their architectures, training objectives, and application domains, highlighting trends and open challenges. Our analysis shows the evolution from traditional feature-based methods to sophisticated cross-encoder models that can capture complex query-document relevance patterns. We conclude with directions for future research and practical deployment considerations.","Published":"2025-04-28T09:15:32Z","PDFURL":"http://arxiv.org/pdf/2504.19876v1","SourceURL":"http://arxiv.org/abs/2504.19876v1","Metadata":{"primary_category":"cs.IR"}},{"Title":"Efficient Cross-Encoders: Knowledge Distillation for Fast and Accurate Reranking","Authors":["Michael Zhang","Ling Xu","David Miller"],"Abstract":"Cross-encoders achieve state-of-the-art performance in reranking tasks but suffer from high computational costs during inference, limiting their practical applications. We propose a knowledge distillation approach that transfers knowledge from large cross-encoder models to smaller, more efficient architectures without significant performance degradation. Our method employs a novel distillation objective that focuses on preserving ranking order rather than exact score matching. Experiments on MS MARCO, TREC-COVID, and ArXiv datasets show that our distilled models achieve up to 5x speedup with less than 2% reduction in effectiveness metrics. We also introduce a dynamic inference mechanism that adjusts computation based on query complexity, further improving efficiency for real-world applications.","Published":"2025-05-01T14:22:11Z","PDFURL":"http://arxiv.org/pdf/2505.00723v1","SourceURL":"http://arxiv.org/abs/2505.00723v1","Metadata":{"primary_category":"cs.IR"}},{"Title":"Transformer-based Cross-encoders for Multilingual Academic Paper Ranking","Authors":["Akira Tanaka","Marie Dubois","Hassan Ahmed"],"Abstract":"Academic search engines must effectively rank papers across multiple languages to serve the global research community. We introduce MultiScholar, a transformer-based cross-encoder model specifically designed for multilingual academic paper ranking. Our model combines multilingual pretraining with a novel contrastive learning objective that emphasizes disciplinary relevance across language boundaries. Evaluations on papers from arXiv, PubMed, and multiple non-English repositories demonstrate that MultiScholar significantly outperforms existing approaches on cross-lingual ranking tasks. We also analyze how domain-specific terminology impacts cross-lingual ranking performance and propose methods to mitigate these challenges.","Published":"2025-04-15T08:45:30Z","PDFURL":"http://arxiv.org/pdf/2504.08214v1","SourceURL":"http://arxiv.org/abs/2504.08214v1","Metadata":{"primary_category":"cs.CL"}},{"Title":"Beyond Relevance: Cross-Encoders for Citation Recommendation in Scientific Literature","Authors":["Carlos Mendez","Priya Sharma","Thomas Johnson"],"Abstract":"Citation recommendation systems help researchers discover relevant prior work, but traditional approaches often focus solely on topical relevance. We propose CitationEncoder, a cross-encoder model that considers multiple facets of citation utility: topical relevance, methodological similarity, complementary findings, and citation influence. Our model combines transformer-based representations with citation graph information to better capture the scholarly import of potential citations. Evaluations on computer science and biomedical literature demonstrate substantial improvements over existing recommendation systems. Additionally, we show that our multi-faceted approach leads to more diverse citation recommendations, potentially reducing citation bias in scholarly work.","Published":"2025-03-22T11:55:18Z","PDFURL":"http://arxiv.org/pdf/2503.14729v2","SourceURL":"http://arxiv.org/abs/2503.14729v2","Metadata":{"primary_category":"cs.DL"}}],"top_n":10}
2025-05-10 18:19:34,910 - arxiv_reranker - DEBUG - Reranking request received with query: 'cross encoders for neural retrieval' for 5 papers
2025-05-10 18:19:34,911 - arxiv_reranker - DEBUG - Request details: query='cross encoders for neural retrieval' results=[ArxivPaper(Title='Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models', Authors=['Oliver Savolainen', 'Dur e Najaf Amjad', 'Roxana Petcu'], Abstract="This reproducibility study analyzes and extends the paper 'Interpreting Content-Based Neural Retrieval Models: What do they learn and when do they fail?' (Faggioli et al., 2022). We focus on investigating how neural retrieval models perform across different languages and document lengths. Our findings confirm the original paper's conclusions that neural models prioritize term frequency and document length features, but we extend this understanding to show how these models struggle with very long documents and exhibit biases in multilingual settings. This work contributes to improving information retrieval systems' performance across diverse content scenarios.", Published='2025-05-04T15:30:45Z', DOI='', PDFURL='http://arxiv.org/pdf/2505.02154v1', SourceURL='http://arxiv.org/abs/2505.02154v1', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.IR'}), ArxivPaper(Title='Reranking for Neural Information Retrieval: A Comprehensive Survey of Approaches and Applications', Authors=['Emma Chen', 'Javier Rodriguez', 'Sarah Johnson'], Abstract='Reranking is a crucial step in modern information retrieval systems, significantly improving the quality of search results by refining an initial candidate set. This survey provides a systematic overview of neural reranking approaches, from early neural network models to recent pretrained language model-based techniques. We categorize reranking approaches based on their architectures, training objectives, and application domains, highlighting trends and open challenges. Our analysis shows the evolution from traditional feature-based methods to sophisticated cross-encoder models that can capture complex query-document relevance patterns. We conclude with directions for future research and practical deployment considerations.', Published='2025-04-28T09:15:32Z', DOI='', PDFURL='http://arxiv.org/pdf/2504.19876v1', SourceURL='http://arxiv.org/abs/2504.19876v1', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.IR'}), ArxivPaper(Title='Efficient Cross-Encoders: Knowledge Distillation for Fast and Accurate Reranking', Authors=['Michael Zhang', 'Ling Xu', 'David Miller'], Abstract='Cross-encoders achieve state-of-the-art performance in reranking tasks but suffer from high computational costs during inference, limiting their practical applications. We propose a knowledge distillation approach that transfers knowledge from large cross-encoder models to smaller, more efficient architectures without significant performance degradation. Our method employs a novel distillation objective that focuses on preserving ranking order rather than exact score matching. Experiments on MS MARCO, TREC-COVID, and ArXiv datasets show that our distilled models achieve up to 5x speedup with less than 2% reduction in effectiveness metrics. We also introduce a dynamic inference mechanism that adjusts computation based on query complexity, further improving efficiency for real-world applications.', Published='2025-05-01T14:22:11Z', DOI='', PDFURL='http://arxiv.org/pdf/2505.00723v1', SourceURL='http://arxiv.org/abs/2505.00723v1', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.IR'}), ArxivPaper(Title='Transformer-based Cross-encoders for Multilingual Academic Paper Ranking', Authors=['Akira Tanaka', 'Marie Dubois', 'Hassan Ahmed'], Abstract='Academic search engines must effectively rank papers across multiple languages to serve the global research community. We introduce MultiScholar, a transformer-based cross-encoder model specifically designed for multilingual academic paper ranking. Our model combines multilingual pretraining with a novel contrastive learning objective that emphasizes disciplinary relevance across language boundaries. Evaluations on papers from arXiv, PubMed, and multiple non-English repositories demonstrate that MultiScholar significantly outperforms existing approaches on cross-lingual ranking tasks. We also analyze how domain-specific terminology impacts cross-lingual ranking performance and propose methods to mitigate these challenges.', Published='2025-04-15T08:45:30Z', DOI='', PDFURL='http://arxiv.org/pdf/2504.08214v1', SourceURL='http://arxiv.org/abs/2504.08214v1', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.CL'}), ArxivPaper(Title='Beyond Relevance: Cross-Encoders for Citation Recommendation in Scientific Literature', Authors=['Carlos Mendez', 'Priya Sharma', 'Thomas Johnson'], Abstract='Citation recommendation systems help researchers discover relevant prior work, but traditional approaches often focus solely on topical relevance. We propose CitationEncoder, a cross-encoder model that considers multiple facets of citation utility: topical relevance, methodological similarity, complementary findings, and citation influence. Our model combines transformer-based representations with citation graph information to better capture the scholarly import of potential citations. Evaluations on computer science and biomedical literature demonstrate substantial improvements over existing recommendation systems. Additionally, we show that our multi-faceted approach leads to more diverse citation recommendations, potentially reducing citation bias in scholarly work.', Published='2025-03-22T11:55:18Z', DOI='', PDFURL='http://arxiv.org/pdf/2503.14729v2', SourceURL='http://arxiv.org/abs/2503.14729v2', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.DL'})] top_n=10
2025-05-10 18:19:34,911 - arxiv_reranker - DEBUG - Prepared 5 paper-query pairs for cross-encoder scoring
2025-05-10 18:19:35,035 - arxiv_reranker - DEBUG - Cross-encoder scoring complete. Score range: -1.69 to 4.79
2025-05-10 18:19:35,035 - arxiv_reranker - DEBUG - Returning top 5 results. Top score: 4.79
2025-05-10 18:19:53,447 - arxiv_reranker - DEBUG - Request body: {"query":"many languages and computers","results":[{"Title":"Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models","Authors":["Oliver Savolainen","Dur e Najaf Amjad","Roxana Petcu"],"Abstract":"This reproducibility study analyzes and extends the paper 'Interpreting Content-Based Neural Retrieval Models: What do they learn and when do they fail?' (Faggioli et al., 2022). We focus on investigating how neural retrieval models perform across different languages and document lengths. Our findings confirm the original paper's conclusions that neural models prioritize term frequency and document length features, but we extend this understanding to show how these models struggle with very long documents and exhibit biases in multilingual settings. This work contributes to improving information retrieval systems' performance across diverse content scenarios.","Published":"2025-05-04T15:30:45Z","PDFURL":"http://arxiv.org/pdf/2505.02154v1","SourceURL":"http://arxiv.org/abs/2505.02154v1","Metadata":{"primary_category":"cs.IR"}},{"Title":"Reranking for Neural Information Retrieval: A Comprehensive Survey of Approaches and Applications","Authors":["Emma Chen","Javier Rodriguez","Sarah Johnson"],"Abstract":"Reranking is a crucial step in modern information retrieval systems, significantly improving the quality of search results by refining an initial candidate set. This survey provides a systematic overview of neural reranking approaches, from early neural network models to recent pretrained language model-based techniques. We categorize reranking approaches based on their architectures, training objectives, and application domains, highlighting trends and open challenges. Our analysis shows the evolution from traditional feature-based methods to sophisticated cross-encoder models that can capture complex query-document relevance patterns. We conclude with directions for future research and practical deployment considerations.","Published":"2025-04-28T09:15:32Z","PDFURL":"http://arxiv.org/pdf/2504.19876v1","SourceURL":"http://arxiv.org/abs/2504.19876v1","Metadata":{"primary_category":"cs.IR"}},{"Title":"Efficient Cross-Encoders: Knowledge Distillation for Fast and Accurate Reranking","Authors":["Michael Zhang","Ling Xu","David Miller"],"Abstract":"Cross-encoders achieve state-of-the-art performance in reranking tasks but suffer from high computational costs during inference, limiting their practical applications. We propose a knowledge distillation approach that transfers knowledge from large cross-encoder models to smaller, more efficient architectures without significant performance degradation. Our method employs a novel distillation objective that focuses on preserving ranking order rather than exact score matching. Experiments on MS MARCO, TREC-COVID, and ArXiv datasets show that our distilled models achieve up to 5x speedup with less than 2% reduction in effectiveness metrics. We also introduce a dynamic inference mechanism that adjusts computation based on query complexity, further improving efficiency for real-world applications.","Published":"2025-05-01T14:22:11Z","PDFURL":"http://arxiv.org/pdf/2505.00723v1","SourceURL":"http://arxiv.org/abs/2505.00723v1","Metadata":{"primary_category":"cs.IR"}},{"Title":"Transformer-based Cross-encoders for Multilingual Academic Paper Ranking","Authors":["Akira Tanaka","Marie Dubois","Hassan Ahmed"],"Abstract":"Academic search engines must effectively rank papers across multiple languages to serve the global research community. We introduce MultiScholar, a transformer-based cross-encoder model specifically designed for multilingual academic paper ranking. Our model combines multilingual pretraining with a novel contrastive learning objective that emphasizes disciplinary relevance across language boundaries. Evaluations on papers from arXiv, PubMed, and multiple non-English repositories demonstrate that MultiScholar significantly outperforms existing approaches on cross-lingual ranking tasks. We also analyze how domain-specific terminology impacts cross-lingual ranking performance and propose methods to mitigate these challenges.","Published":"2025-04-15T08:45:30Z","PDFURL":"http://arxiv.org/pdf/2504.08214v1","SourceURL":"http://arxiv.org/abs/2504.08214v1","Metadata":{"primary_category":"cs.CL"}},{"Title":"Beyond Relevance: Cross-Encoders for Citation Recommendation in Scientific Literature","Authors":["Carlos Mendez","Priya Sharma","Thomas Johnson"],"Abstract":"Citation recommendation systems help researchers discover relevant prior work, but traditional approaches often focus solely on topical relevance. We propose CitationEncoder, a cross-encoder model that considers multiple facets of citation utility: topical relevance, methodological similarity, complementary findings, and citation influence. Our model combines transformer-based representations with citation graph information to better capture the scholarly import of potential citations. Evaluations on computer science and biomedical literature demonstrate substantial improvements over existing recommendation systems. Additionally, we show that our multi-faceted approach leads to more diverse citation recommendations, potentially reducing citation bias in scholarly work.","Published":"2025-03-22T11:55:18Z","PDFURL":"http://arxiv.org/pdf/2503.14729v2","SourceURL":"http://arxiv.org/abs/2503.14729v2","Metadata":{"primary_category":"cs.DL"}}],"top_n":10}
2025-05-10 18:19:53,448 - arxiv_reranker - DEBUG - Reranking request received with query: 'many languages and computers' for 5 papers
2025-05-10 18:19:53,448 - arxiv_reranker - DEBUG - Request details: query='many languages and computers' results=[ArxivPaper(Title='Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models', Authors=['Oliver Savolainen', 'Dur e Najaf Amjad', 'Roxana Petcu'], Abstract="This reproducibility study analyzes and extends the paper 'Interpreting Content-Based Neural Retrieval Models: What do they learn and when do they fail?' (Faggioli et al., 2022). We focus on investigating how neural retrieval models perform across different languages and document lengths. Our findings confirm the original paper's conclusions that neural models prioritize term frequency and document length features, but we extend this understanding to show how these models struggle with very long documents and exhibit biases in multilingual settings. This work contributes to improving information retrieval systems' performance across diverse content scenarios.", Published='2025-05-04T15:30:45Z', DOI='', PDFURL='http://arxiv.org/pdf/2505.02154v1', SourceURL='http://arxiv.org/abs/2505.02154v1', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.IR'}), ArxivPaper(Title='Reranking for Neural Information Retrieval: A Comprehensive Survey of Approaches and Applications', Authors=['Emma Chen', 'Javier Rodriguez', 'Sarah Johnson'], Abstract='Reranking is a crucial step in modern information retrieval systems, significantly improving the quality of search results by refining an initial candidate set. This survey provides a systematic overview of neural reranking approaches, from early neural network models to recent pretrained language model-based techniques. We categorize reranking approaches based on their architectures, training objectives, and application domains, highlighting trends and open challenges. Our analysis shows the evolution from traditional feature-based methods to sophisticated cross-encoder models that can capture complex query-document relevance patterns. We conclude with directions for future research and practical deployment considerations.', Published='2025-04-28T09:15:32Z', DOI='', PDFURL='http://arxiv.org/pdf/2504.19876v1', SourceURL='http://arxiv.org/abs/2504.19876v1', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.IR'}), ArxivPaper(Title='Efficient Cross-Encoders: Knowledge Distillation for Fast and Accurate Reranking', Authors=['Michael Zhang', 'Ling Xu', 'David Miller'], Abstract='Cross-encoders achieve state-of-the-art performance in reranking tasks but suffer from high computational costs during inference, limiting their practical applications. We propose a knowledge distillation approach that transfers knowledge from large cross-encoder models to smaller, more efficient architectures without significant performance degradation. Our method employs a novel distillation objective that focuses on preserving ranking order rather than exact score matching. Experiments on MS MARCO, TREC-COVID, and ArXiv datasets show that our distilled models achieve up to 5x speedup with less than 2% reduction in effectiveness metrics. We also introduce a dynamic inference mechanism that adjusts computation based on query complexity, further improving efficiency for real-world applications.', Published='2025-05-01T14:22:11Z', DOI='', PDFURL='http://arxiv.org/pdf/2505.00723v1', SourceURL='http://arxiv.org/abs/2505.00723v1', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.IR'}), ArxivPaper(Title='Transformer-based Cross-encoders for Multilingual Academic Paper Ranking', Authors=['Akira Tanaka', 'Marie Dubois', 'Hassan Ahmed'], Abstract='Academic search engines must effectively rank papers across multiple languages to serve the global research community. We introduce MultiScholar, a transformer-based cross-encoder model specifically designed for multilingual academic paper ranking. Our model combines multilingual pretraining with a novel contrastive learning objective that emphasizes disciplinary relevance across language boundaries. Evaluations on papers from arXiv, PubMed, and multiple non-English repositories demonstrate that MultiScholar significantly outperforms existing approaches on cross-lingual ranking tasks. We also analyze how domain-specific terminology impacts cross-lingual ranking performance and propose methods to mitigate these challenges.', Published='2025-04-15T08:45:30Z', DOI='', PDFURL='http://arxiv.org/pdf/2504.08214v1', SourceURL='http://arxiv.org/abs/2504.08214v1', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.CL'}), ArxivPaper(Title='Beyond Relevance: Cross-Encoders for Citation Recommendation in Scientific Literature', Authors=['Carlos Mendez', 'Priya Sharma', 'Thomas Johnson'], Abstract='Citation recommendation systems help researchers discover relevant prior work, but traditional approaches often focus solely on topical relevance. We propose CitationEncoder, a cross-encoder model that considers multiple facets of citation utility: topical relevance, methodological similarity, complementary findings, and citation influence. Our model combines transformer-based representations with citation graph information to better capture the scholarly import of potential citations. Evaluations on computer science and biomedical literature demonstrate substantial improvements over existing recommendation systems. Additionally, we show that our multi-faceted approach leads to more diverse citation recommendations, potentially reducing citation bias in scholarly work.', Published='2025-03-22T11:55:18Z', DOI='', PDFURL='http://arxiv.org/pdf/2503.14729v2', SourceURL='http://arxiv.org/abs/2503.14729v2', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.DL'})] top_n=10
2025-05-10 18:19:53,448 - arxiv_reranker - DEBUG - Prepared 5 paper-query pairs for cross-encoder scoring
2025-05-10 18:19:53,561 - arxiv_reranker - DEBUG - Cross-encoder scoring complete. Score range: -10.97 to -9.55
2025-05-10 18:19:53,561 - arxiv_reranker - DEBUG - Returning top 5 results. Top score: -9.55
2025-05-10 18:20:04,183 - arxiv_reranker - DEBUG - Request body: {"query":"many languages and universities","results":[{"Title":"Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models","Authors":["Oliver Savolainen","Dur e Najaf Amjad","Roxana Petcu"],"Abstract":"This reproducibility study analyzes and extends the paper 'Interpreting Content-Based Neural Retrieval Models: What do they learn and when do they fail?' (Faggioli et al., 2022). We focus on investigating how neural retrieval models perform across different languages and document lengths. Our findings confirm the original paper's conclusions that neural models prioritize term frequency and document length features, but we extend this understanding to show how these models struggle with very long documents and exhibit biases in multilingual settings. This work contributes to improving information retrieval systems' performance across diverse content scenarios.","Published":"2025-05-04T15:30:45Z","PDFURL":"http://arxiv.org/pdf/2505.02154v1","SourceURL":"http://arxiv.org/abs/2505.02154v1","Metadata":{"primary_category":"cs.IR"}},{"Title":"Reranking for Neural Information Retrieval: A Comprehensive Survey of Approaches and Applications","Authors":["Emma Chen","Javier Rodriguez","Sarah Johnson"],"Abstract":"Reranking is a crucial step in modern information retrieval systems, significantly improving the quality of search results by refining an initial candidate set. This survey provides a systematic overview of neural reranking approaches, from early neural network models to recent pretrained language model-based techniques. We categorize reranking approaches based on their architectures, training objectives, and application domains, highlighting trends and open challenges. Our analysis shows the evolution from traditional feature-based methods to sophisticated cross-encoder models that can capture complex query-document relevance patterns. We conclude with directions for future research and practical deployment considerations.","Published":"2025-04-28T09:15:32Z","PDFURL":"http://arxiv.org/pdf/2504.19876v1","SourceURL":"http://arxiv.org/abs/2504.19876v1","Metadata":{"primary_category":"cs.IR"}},{"Title":"Efficient Cross-Encoders: Knowledge Distillation for Fast and Accurate Reranking","Authors":["Michael Zhang","Ling Xu","David Miller"],"Abstract":"Cross-encoders achieve state-of-the-art performance in reranking tasks but suffer from high computational costs during inference, limiting their practical applications. We propose a knowledge distillation approach that transfers knowledge from large cross-encoder models to smaller, more efficient architectures without significant performance degradation. Our method employs a novel distillation objective that focuses on preserving ranking order rather than exact score matching. Experiments on MS MARCO, TREC-COVID, and ArXiv datasets show that our distilled models achieve up to 5x speedup with less than 2% reduction in effectiveness metrics. We also introduce a dynamic inference mechanism that adjusts computation based on query complexity, further improving efficiency for real-world applications.","Published":"2025-05-01T14:22:11Z","PDFURL":"http://arxiv.org/pdf/2505.00723v1","SourceURL":"http://arxiv.org/abs/2505.00723v1","Metadata":{"primary_category":"cs.IR"}},{"Title":"Transformer-based Cross-encoders for Multilingual Academic Paper Ranking","Authors":["Akira Tanaka","Marie Dubois","Hassan Ahmed"],"Abstract":"Academic search engines must effectively rank papers across multiple languages to serve the global research community. We introduce MultiScholar, a transformer-based cross-encoder model specifically designed for multilingual academic paper ranking. Our model combines multilingual pretraining with a novel contrastive learning objective that emphasizes disciplinary relevance across language boundaries. Evaluations on papers from arXiv, PubMed, and multiple non-English repositories demonstrate that MultiScholar significantly outperforms existing approaches on cross-lingual ranking tasks. We also analyze how domain-specific terminology impacts cross-lingual ranking performance and propose methods to mitigate these challenges.","Published":"2025-04-15T08:45:30Z","PDFURL":"http://arxiv.org/pdf/2504.08214v1","SourceURL":"http://arxiv.org/abs/2504.08214v1","Metadata":{"primary_category":"cs.CL"}},{"Title":"Beyond Relevance: Cross-Encoders for Citation Recommendation in Scientific Literature","Authors":["Carlos Mendez","Priya Sharma","Thomas Johnson"],"Abstract":"Citation recommendation systems help researchers discover relevant prior work, but traditional approaches often focus solely on topical relevance. We propose CitationEncoder, a cross-encoder model that considers multiple facets of citation utility: topical relevance, methodological similarity, complementary findings, and citation influence. Our model combines transformer-based representations with citation graph information to better capture the scholarly import of potential citations. Evaluations on computer science and biomedical literature demonstrate substantial improvements over existing recommendation systems. Additionally, we show that our multi-faceted approach leads to more diverse citation recommendations, potentially reducing citation bias in scholarly work.","Published":"2025-03-22T11:55:18Z","PDFURL":"http://arxiv.org/pdf/2503.14729v2","SourceURL":"http://arxiv.org/abs/2503.14729v2","Metadata":{"primary_category":"cs.DL"}}],"top_n":10}
2025-05-10 18:20:04,184 - arxiv_reranker - DEBUG - Reranking request received with query: 'many languages and universities' for 5 papers
2025-05-10 18:20:04,184 - arxiv_reranker - DEBUG - Request details: query='many languages and universities' results=[ArxivPaper(Title='Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models', Authors=['Oliver Savolainen', 'Dur e Najaf Amjad', 'Roxana Petcu'], Abstract="This reproducibility study analyzes and extends the paper 'Interpreting Content-Based Neural Retrieval Models: What do they learn and when do they fail?' (Faggioli et al., 2022). We focus on investigating how neural retrieval models perform across different languages and document lengths. Our findings confirm the original paper's conclusions that neural models prioritize term frequency and document length features, but we extend this understanding to show how these models struggle with very long documents and exhibit biases in multilingual settings. This work contributes to improving information retrieval systems' performance across diverse content scenarios.", Published='2025-05-04T15:30:45Z', DOI='', PDFURL='http://arxiv.org/pdf/2505.02154v1', SourceURL='http://arxiv.org/abs/2505.02154v1', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.IR'}), ArxivPaper(Title='Reranking for Neural Information Retrieval: A Comprehensive Survey of Approaches and Applications', Authors=['Emma Chen', 'Javier Rodriguez', 'Sarah Johnson'], Abstract='Reranking is a crucial step in modern information retrieval systems, significantly improving the quality of search results by refining an initial candidate set. This survey provides a systematic overview of neural reranking approaches, from early neural network models to recent pretrained language model-based techniques. We categorize reranking approaches based on their architectures, training objectives, and application domains, highlighting trends and open challenges. Our analysis shows the evolution from traditional feature-based methods to sophisticated cross-encoder models that can capture complex query-document relevance patterns. We conclude with directions for future research and practical deployment considerations.', Published='2025-04-28T09:15:32Z', DOI='', PDFURL='http://arxiv.org/pdf/2504.19876v1', SourceURL='http://arxiv.org/abs/2504.19876v1', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.IR'}), ArxivPaper(Title='Efficient Cross-Encoders: Knowledge Distillation for Fast and Accurate Reranking', Authors=['Michael Zhang', 'Ling Xu', 'David Miller'], Abstract='Cross-encoders achieve state-of-the-art performance in reranking tasks but suffer from high computational costs during inference, limiting their practical applications. We propose a knowledge distillation approach that transfers knowledge from large cross-encoder models to smaller, more efficient architectures without significant performance degradation. Our method employs a novel distillation objective that focuses on preserving ranking order rather than exact score matching. Experiments on MS MARCO, TREC-COVID, and ArXiv datasets show that our distilled models achieve up to 5x speedup with less than 2% reduction in effectiveness metrics. We also introduce a dynamic inference mechanism that adjusts computation based on query complexity, further improving efficiency for real-world applications.', Published='2025-05-01T14:22:11Z', DOI='', PDFURL='http://arxiv.org/pdf/2505.00723v1', SourceURL='http://arxiv.org/abs/2505.00723v1', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.IR'}), ArxivPaper(Title='Transformer-based Cross-encoders for Multilingual Academic Paper Ranking', Authors=['Akira Tanaka', 'Marie Dubois', 'Hassan Ahmed'], Abstract='Academic search engines must effectively rank papers across multiple languages to serve the global research community. We introduce MultiScholar, a transformer-based cross-encoder model specifically designed for multilingual academic paper ranking. Our model combines multilingual pretraining with a novel contrastive learning objective that emphasizes disciplinary relevance across language boundaries. Evaluations on papers from arXiv, PubMed, and multiple non-English repositories demonstrate that MultiScholar significantly outperforms existing approaches on cross-lingual ranking tasks. We also analyze how domain-specific terminology impacts cross-lingual ranking performance and propose methods to mitigate these challenges.', Published='2025-04-15T08:45:30Z', DOI='', PDFURL='http://arxiv.org/pdf/2504.08214v1', SourceURL='http://arxiv.org/abs/2504.08214v1', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.CL'}), ArxivPaper(Title='Beyond Relevance: Cross-Encoders for Citation Recommendation in Scientific Literature', Authors=['Carlos Mendez', 'Priya Sharma', 'Thomas Johnson'], Abstract='Citation recommendation systems help researchers discover relevant prior work, but traditional approaches often focus solely on topical relevance. We propose CitationEncoder, a cross-encoder model that considers multiple facets of citation utility: topical relevance, methodological similarity, complementary findings, and citation influence. Our model combines transformer-based representations with citation graph information to better capture the scholarly import of potential citations. Evaluations on computer science and biomedical literature demonstrate substantial improvements over existing recommendation systems. Additionally, we show that our multi-faceted approach leads to more diverse citation recommendations, potentially reducing citation bias in scholarly work.', Published='2025-03-22T11:55:18Z', DOI='', PDFURL='http://arxiv.org/pdf/2503.14729v2', SourceURL='http://arxiv.org/abs/2503.14729v2', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.DL'})] top_n=10
2025-05-10 18:20:04,184 - arxiv_reranker - DEBUG - Prepared 5 paper-query pairs for cross-encoder scoring
2025-05-10 18:20:04,267 - arxiv_reranker - DEBUG - Cross-encoder scoring complete. Score range: -11.00 to -6.93
2025-05-10 18:20:04,267 - arxiv_reranker - DEBUG - Returning top 5 results. Top score: -6.93
2025-05-10 18:20:13,792 - arxiv_reranker - DEBUG - Request body: {"query":"many languages and long texts","results":[{"Title":"Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models","Authors":["Oliver Savolainen","Dur e Najaf Amjad","Roxana Petcu"],"Abstract":"This reproducibility study analyzes and extends the paper 'Interpreting Content-Based Neural Retrieval Models: What do they learn and when do they fail?' (Faggioli et al., 2022). We focus on investigating how neural retrieval models perform across different languages and document lengths. Our findings confirm the original paper's conclusions that neural models prioritize term frequency and document length features, but we extend this understanding to show how these models struggle with very long documents and exhibit biases in multilingual settings. This work contributes to improving information retrieval systems' performance across diverse content scenarios.","Published":"2025-05-04T15:30:45Z","PDFURL":"http://arxiv.org/pdf/2505.02154v1","SourceURL":"http://arxiv.org/abs/2505.02154v1","Metadata":{"primary_category":"cs.IR"}},{"Title":"Reranking for Neural Information Retrieval: A Comprehensive Survey of Approaches and Applications","Authors":["Emma Chen","Javier Rodriguez","Sarah Johnson"],"Abstract":"Reranking is a crucial step in modern information retrieval systems, significantly improving the quality of search results by refining an initial candidate set. This survey provides a systematic overview of neural reranking approaches, from early neural network models to recent pretrained language model-based techniques. We categorize reranking approaches based on their architectures, training objectives, and application domains, highlighting trends and open challenges. Our analysis shows the evolution from traditional feature-based methods to sophisticated cross-encoder models that can capture complex query-document relevance patterns. We conclude with directions for future research and practical deployment considerations.","Published":"2025-04-28T09:15:32Z","PDFURL":"http://arxiv.org/pdf/2504.19876v1","SourceURL":"http://arxiv.org/abs/2504.19876v1","Metadata":{"primary_category":"cs.IR"}},{"Title":"Efficient Cross-Encoders: Knowledge Distillation for Fast and Accurate Reranking","Authors":["Michael Zhang","Ling Xu","David Miller"],"Abstract":"Cross-encoders achieve state-of-the-art performance in reranking tasks but suffer from high computational costs during inference, limiting their practical applications. We propose a knowledge distillation approach that transfers knowledge from large cross-encoder models to smaller, more efficient architectures without significant performance degradation. Our method employs a novel distillation objective that focuses on preserving ranking order rather than exact score matching. Experiments on MS MARCO, TREC-COVID, and ArXiv datasets show that our distilled models achieve up to 5x speedup with less than 2% reduction in effectiveness metrics. We also introduce a dynamic inference mechanism that adjusts computation based on query complexity, further improving efficiency for real-world applications.","Published":"2025-05-01T14:22:11Z","PDFURL":"http://arxiv.org/pdf/2505.00723v1","SourceURL":"http://arxiv.org/abs/2505.00723v1","Metadata":{"primary_category":"cs.IR"}},{"Title":"Transformer-based Cross-encoders for Multilingual Academic Paper Ranking","Authors":["Akira Tanaka","Marie Dubois","Hassan Ahmed"],"Abstract":"Academic search engines must effectively rank papers across multiple languages to serve the global research community. We introduce MultiScholar, a transformer-based cross-encoder model specifically designed for multilingual academic paper ranking. Our model combines multilingual pretraining with a novel contrastive learning objective that emphasizes disciplinary relevance across language boundaries. Evaluations on papers from arXiv, PubMed, and multiple non-English repositories demonstrate that MultiScholar significantly outperforms existing approaches on cross-lingual ranking tasks. We also analyze how domain-specific terminology impacts cross-lingual ranking performance and propose methods to mitigate these challenges.","Published":"2025-04-15T08:45:30Z","PDFURL":"http://arxiv.org/pdf/2504.08214v1","SourceURL":"http://arxiv.org/abs/2504.08214v1","Metadata":{"primary_category":"cs.CL"}},{"Title":"Beyond Relevance: Cross-Encoders for Citation Recommendation in Scientific Literature","Authors":["Carlos Mendez","Priya Sharma","Thomas Johnson"],"Abstract":"Citation recommendation systems help researchers discover relevant prior work, but traditional approaches often focus solely on topical relevance. We propose CitationEncoder, a cross-encoder model that considers multiple facets of citation utility: topical relevance, methodological similarity, complementary findings, and citation influence. Our model combines transformer-based representations with citation graph information to better capture the scholarly import of potential citations. Evaluations on computer science and biomedical literature demonstrate substantial improvements over existing recommendation systems. Additionally, we show that our multi-faceted approach leads to more diverse citation recommendations, potentially reducing citation bias in scholarly work.","Published":"2025-03-22T11:55:18Z","PDFURL":"http://arxiv.org/pdf/2503.14729v2","SourceURL":"http://arxiv.org/abs/2503.14729v2","Metadata":{"primary_category":"cs.DL"}}],"top_n":10}
2025-05-10 18:20:13,793 - arxiv_reranker - DEBUG - Reranking request received with query: 'many languages and long texts' for 5 papers
2025-05-10 18:20:13,793 - arxiv_reranker - DEBUG - Request details: query='many languages and long texts' results=[ArxivPaper(Title='Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models', Authors=['Oliver Savolainen', 'Dur e Najaf Amjad', 'Roxana Petcu'], Abstract="This reproducibility study analyzes and extends the paper 'Interpreting Content-Based Neural Retrieval Models: What do they learn and when do they fail?' (Faggioli et al., 2022). We focus on investigating how neural retrieval models perform across different languages and document lengths. Our findings confirm the original paper's conclusions that neural models prioritize term frequency and document length features, but we extend this understanding to show how these models struggle with very long documents and exhibit biases in multilingual settings. This work contributes to improving information retrieval systems' performance across diverse content scenarios.", Published='2025-05-04T15:30:45Z', DOI='', PDFURL='http://arxiv.org/pdf/2505.02154v1', SourceURL='http://arxiv.org/abs/2505.02154v1', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.IR'}), ArxivPaper(Title='Reranking for Neural Information Retrieval: A Comprehensive Survey of Approaches and Applications', Authors=['Emma Chen', 'Javier Rodriguez', 'Sarah Johnson'], Abstract='Reranking is a crucial step in modern information retrieval systems, significantly improving the quality of search results by refining an initial candidate set. This survey provides a systematic overview of neural reranking approaches, from early neural network models to recent pretrained language model-based techniques. We categorize reranking approaches based on their architectures, training objectives, and application domains, highlighting trends and open challenges. Our analysis shows the evolution from traditional feature-based methods to sophisticated cross-encoder models that can capture complex query-document relevance patterns. We conclude with directions for future research and practical deployment considerations.', Published='2025-04-28T09:15:32Z', DOI='', PDFURL='http://arxiv.org/pdf/2504.19876v1', SourceURL='http://arxiv.org/abs/2504.19876v1', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.IR'}), ArxivPaper(Title='Efficient Cross-Encoders: Knowledge Distillation for Fast and Accurate Reranking', Authors=['Michael Zhang', 'Ling Xu', 'David Miller'], Abstract='Cross-encoders achieve state-of-the-art performance in reranking tasks but suffer from high computational costs during inference, limiting their practical applications. We propose a knowledge distillation approach that transfers knowledge from large cross-encoder models to smaller, more efficient architectures without significant performance degradation. Our method employs a novel distillation objective that focuses on preserving ranking order rather than exact score matching. Experiments on MS MARCO, TREC-COVID, and ArXiv datasets show that our distilled models achieve up to 5x speedup with less than 2% reduction in effectiveness metrics. We also introduce a dynamic inference mechanism that adjusts computation based on query complexity, further improving efficiency for real-world applications.', Published='2025-05-01T14:22:11Z', DOI='', PDFURL='http://arxiv.org/pdf/2505.00723v1', SourceURL='http://arxiv.org/abs/2505.00723v1', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.IR'}), ArxivPaper(Title='Transformer-based Cross-encoders for Multilingual Academic Paper Ranking', Authors=['Akira Tanaka', 'Marie Dubois', 'Hassan Ahmed'], Abstract='Academic search engines must effectively rank papers across multiple languages to serve the global research community. We introduce MultiScholar, a transformer-based cross-encoder model specifically designed for multilingual academic paper ranking. Our model combines multilingual pretraining with a novel contrastive learning objective that emphasizes disciplinary relevance across language boundaries. Evaluations on papers from arXiv, PubMed, and multiple non-English repositories demonstrate that MultiScholar significantly outperforms existing approaches on cross-lingual ranking tasks. We also analyze how domain-specific terminology impacts cross-lingual ranking performance and propose methods to mitigate these challenges.', Published='2025-04-15T08:45:30Z', DOI='', PDFURL='http://arxiv.org/pdf/2504.08214v1', SourceURL='http://arxiv.org/abs/2504.08214v1', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.CL'}), ArxivPaper(Title='Beyond Relevance: Cross-Encoders for Citation Recommendation in Scientific Literature', Authors=['Carlos Mendez', 'Priya Sharma', 'Thomas Johnson'], Abstract='Citation recommendation systems help researchers discover relevant prior work, but traditional approaches often focus solely on topical relevance. We propose CitationEncoder, a cross-encoder model that considers multiple facets of citation utility: topical relevance, methodological similarity, complementary findings, and citation influence. Our model combines transformer-based representations with citation graph information to better capture the scholarly import of potential citations. Evaluations on computer science and biomedical literature demonstrate substantial improvements over existing recommendation systems. Additionally, we show that our multi-faceted approach leads to more diverse citation recommendations, potentially reducing citation bias in scholarly work.', Published='2025-03-22T11:55:18Z', DOI='', PDFURL='http://arxiv.org/pdf/2503.14729v2', SourceURL='http://arxiv.org/abs/2503.14729v2', SourceName='arxiv', OAStatus='green', License='', FileSize='', Citations=0, Type='', JournalInfo='', Metadata={'primary_category': 'cs.DL'})] top_n=10
2025-05-10 18:20:13,793 - arxiv_reranker - DEBUG - Prepared 5 paper-query pairs for cross-encoder scoring
2025-05-10 18:20:13,894 - arxiv_reranker - DEBUG - Cross-encoder scoring complete. Score range: -11.19 to -8.53
2025-05-10 18:20:13,894 - arxiv_reranker - DEBUG - Returning top 5 results. Top score: -8.53
